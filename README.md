## üß† Neural Agent v10.0 (Unified)

This project is a sophisticated, modular conversational AI agent designed to learn, remember, and reason using a hybrid of neural (embeddings, classification) and symbolic (rule-based, planning) techniques. It features a robust, multi-layer pipeline architecture for processing user input and generating responses.

-----

### ‚ú® Features

  * **Hybrid Intelligence:** Combines deep learning models (Sentence Transformers, Scikit-learn Classifiers, Hugging Face Generative Models) with a structured, rule-based reasoning engine.
  * **Modular Pipeline:** Processes user input through a seven-layer pipeline: `Input` $\rightarrow$ `Perception` $\rightarrow$ `Understanding` $\rightarrow$ `Context` $\rightarrow$ `Reasoning` $\rightarrow$ `Retrieval` $\rightarrow$ `Execution`.
  * **Persistent Memory:** Uses an SQLite database (`neural_bot_memory.db`) to store:
      * **QA Memory:** Factual knowledge (question/normalized question, answer, embedding).
      * **User Facts:** Key user attributes and preferences (e.g., `name`, `preference:food`).
      * **Conversation Log:** Detailed history of turns and embeddings.
  * **Advanced Retrieval:** Employs a **Hybrid Search** mechanism that weights similarity based on:
    1.  **Semantic Similarity** ($W_{\text{SEMANTIC}}$)
    2.  **Conversational Context** ($W_{\text{CONTEXT}}$)
    3.  **Recency** ($W_{\text{RECENCY}}$)
    4.  **User Profile** ($W_{\text{PROFILE}}$)
  * **Learning & Correction:**
      * **Fact Learning:** Can learn and update user attributes/preferences (e.g., "my name is Bob").
      * **Self-Correction:** Allows users to flag incorrect memories, prompting the agent to learn the correct information.
      * **Teaching Mode:** Falls back to an interactive "Teach me?" mode when an answer is not found in memory or generated by the LLM.
  * **Personality & Linguistics:** Features utility classes for simple paraphrasing, templated responses, and personality formatting to make interactions more engaging.

-----

### üõ†Ô∏è Setup and Installation

#### Prerequisites

  * Python 3.7+

#### Full AI Mode (Recommended)

To run the agent with full neural capabilities (embeddings, classification, and generative AI):

```bash
pip install sentence-transformers scikit-learn numpy transformers sqlite3
```

This will enable `SentenceTransformer` for embeddings, `sklearn` for intent/act classification, and `transformers` (using `google/flan-t5-small`) for generation/synthesis.

#### Light Mode (Rule-based only)

If the AI libraries are not installed, the bot will default to **LIGHT MODE**, which uses dummy vectors and relies only on rule-based processing (greetings, explicit fact patterns, metadata commands).

-----

### üöÄ Usage

1.  **Run the script:**

    ```bash
    python main.py
    ```

2.  **Interact with the agent:**

    The agent will prompt you with ` You:  ` or, if in correction/teaching mode, ` You (Teaching):  `.

    **Example Interactions:**

    | Type | User Input | Agent Action |
    | :--- | :--- | :--- |
    | **Teach** | `Jupiter is the biggest planet` | Saves memory to DB. |
    | **Query** | `What is Jupiter?` | Retrieves answer from DB using Hybrid Search. |
    | **Fact** | `My name is Alex` | Saves `name:Alex` to `user_facts`. |
    | **Correct** | `That's wrong.` (after a query) | Flags the memory and enters Teaching Mode. |
    | **Meta** | `wipe data` | Deletes all records from the database. |

### ‚öôÔ∏è Configuration (Key Variables)

The following variables in `main.py` control the core behavior and performance:

| Variable | Description | Default Value |
| :--- | :--- | :--- |
| `EMBEDDING_MODEL_NAME` | Model for creating sentence embeddings. | `'all-MiniLM-L6-v2'` |
| `GENERATIVE_MODEL_NAME` | Hugging Face model for GenAI fallback/synthesis. | `'google/flan-t5-small'` |
| `CONTEXT_TIMEOUT_SECONDS` | Time limit for an `active_topic` to remain in context. | `300` (5 minutes) |
| `W_SEMANTIC`, `W_CONTEXT`, etc. | Weights for the Hybrid Search algorithm (must sum to 1.0). | $0.5, 0.2, 0.1, 0.2$ |
| `CONFIDENCE_THRESHOLDS` | Scores defining the quality of a retrieval result. | High: $0.85$, Low: $0.60$ |

-----

### üèóÔ∏è Architecture Layers

The `PipelineBot` orchestrates the following layers in sequence:

| Layer | Responsibility | Key Output |
| :--- | :--- | :--- |
| **InputLayer** | Pre-processing, safety checks, normalization. | `state.norm_text` |
| **PerceptionLayer** | Feature extraction and embedding. | `state.raw_vector`, `state.features` |
| **UnderstandingLayer** | Intent and Act classification using $\text{CalibratedClassifierCV}$. | `state.intent`, `state.act` |
| **ContextLayer** | Manages `UserProfile` and `WorkingMemory` (topics, history). | `state.user_profile`, `state.weighted_context` |
| **ReasoningLayer** | The planner‚Äîdecides the necessary action(s) based on intent, act, and features. | `state.plan` (List of operations) |
| **RetrievalLayer** | Executes Hybrid Search against QA memory; falls back to LLM/Teaching. | Modifies `state.plan` with concrete response operations. |
| **ExecutionLayer** | Runs the final operations in the plan (respond, save fact, wipe, teach mode). | `state.response`, updates memory/DB. |

-----

### üíæ Data Persistence

All long-term knowledge, including the trained classifiers and memory vectors (PCA, Topic Model), are saved as pickle files (`.pkl`) and SQLite database (`.db`) to allow the bot to retain knowledge between sessions.

-----

### ‚ö†Ô∏è Light Mode Limitations

In Light Mode, the following features are disabled:

  * Semantic (vector-based) search.
  * Intent and Act classification (defaults to rule-based or `UNKNOWN`).
  * Generative AI fallback and synthesis.
  * Hybrid Search weighting (no contextual, recency, or profile scoring).

Would you like me to elaborate on a specific component of the agent, such as the **Hybrid Search** or the **Intent Classification** process?


-- autogenerated using ai
